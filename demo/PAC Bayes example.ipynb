{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from dhmc.dhmc_sampler import DHMCSampler\n",
    "from dhmc.mcmc_diagnostic import batch_ess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the (cleaned) SECOM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('secom_outcome.npy')\n",
    "X = np.load('secom_design_matrix.npy') # With intercept.\n",
    "n_param = X.shape[1]\n",
    "n_disc = n_param # No conditional density is smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to compute the posterior density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta, req_grad=True):\n",
    "    \"\"\"\n",
    "    Computes the log posterior density and its gradient. \n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    theta : ndarray\n",
    "    req_grad : bool\n",
    "        If True, returns the gradient along with the log density.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    logp : float\n",
    "    grad : ndarray\n",
    "    aux : Any\n",
    "        Any computed quantities that can be re-used by the \n",
    "        subsequent calls to the function 'f_updated' to save on\n",
    "        computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    logp = 0\n",
    "    grad = np.zeros(len(y))\n",
    "    \n",
    "    # Contribution from the prior.\n",
    "    logp += - np.sum(theta ** 2) / 2\n",
    "    \n",
    "    # Contribution from the likelihood.\n",
    "    y_hat = np.dot(X, theta)\n",
    "    loglik = np.count_nonzero(y * y_hat > 0)\n",
    "    logp += loglik\n",
    "    \n",
    "    aux = (loglik, y_hat)\n",
    "    return logp, np.zeros(len(theta)), aux\n",
    "\n",
    "def f_update(theta, dtheta, j, aux):\n",
    "    \"\"\"\n",
    "    Computes the difference in the log conditional density \n",
    "    along a given parameter index 'j'.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    theta : ndarray\n",
    "    dtheta : float\n",
    "        Amount by which the j-th parameter is updated.\n",
    "    j : int\n",
    "        Index of the parameter to update.\n",
    "    aux : Any\n",
    "        Computed quantities from the most recent call to functions\n",
    "        'f' or 'f_update' that can be re-used to save on computation.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    logp_diff : float\n",
    "    aux_new : Any\n",
    "    \"\"\"\n",
    "    \n",
    "    loglik_prev, y_hat = aux\n",
    "    y_hat = y_hat + X[:,j] * dtheta\n",
    "    \n",
    "    logp_diff = (theta[j] ** 2 - (theta[j] + dtheta) ** 2) / 2\n",
    "    \n",
    "    # Contribution from the likelihood.\n",
    "    loglik = np.count_nonzero(y * y_hat > 0)\n",
    "    logp_diff += loglik - loglik_prev\n",
    "    \n",
    "    aux_new = (loglik, y_hat)\n",
    "    return logp_diff, aux_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial state for MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept0 = np.log(np.mean(y == 1) / (1 - np.mean(y == 1)))\n",
    "beta0 = np.zeros(X.shape[1])\n",
    "beta0[0] = intercept0\n",
    "theta0 = beta0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the gradient and updating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.ones(n_param)\n",
    "dhmc = DHMCSampler(f, f_update, n_disc, n_param, scale)\n",
    "dhmc.test_cont_grad(theta0, sd=.01, n_test=10);\n",
    "_, theta, logp_fdiff, logp_diff = \\\n",
    "    dhmc.test_update(theta0, sd=10, n_test=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run discontinuous HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_burnin = 10 ** 2\n",
    "n_sample = 10 ** 3\n",
    "dt = .3 * np.array([.7, 1]) \n",
    "nstep = [20, 30] \n",
    "samples, logp_samples, accept_prob, nfevals_per_itr, time_elapsed = \\\n",
    "    dhmc.run_sampler(theta0, dt, nstep, n_burnin, n_sample, seed=seed)\n",
    "    \n",
    "dhmc_samples = samples[n_burnin:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ESS's and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_dhmc = batch_ess(dhmc_samples, normed=False)\n",
    "index_sort = np.argsort(ess_dhmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ess_dhmc[index_sort[:100]])\n",
    "plt.ylabel('ESS')\n",
    "plt.xlabel('Param index (sorted)')\n",
    "plt.title('ESS of 100 worst mixing params')\n",
    "plt.ylim(0, 1.05 * np.max(ess_dhmc[index_sort[:100]]))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dhmc_samples[:, index_sort[:3]])\n",
    "plt.ylabel('Param values')\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.title('Traceplot of 3 worst mixing params')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Metropolis-within-Gibbs for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate proposal variances are adapted to achieve .441 acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_metropolis_gibbs(theta, prop_sd, aux, n_adap, n_per_adap=10):\n",
    "    accept_rate = np.zeros((n_adap, n_param))\n",
    "    for i in range(n_adap):\n",
    "        adapt_rate = (i + 1) ** -.5\n",
    "        theta, prop_sd, accept_rate[i,:], aux \\\n",
    "            = adap_metropolis_gibbs_step(theta, prop_sd, aux, adapt_rate, n_per_adap)\n",
    "    return theta, accept_rate, prop_sd, aux  \n",
    "\n",
    "def adap_metropolis_gibbs_step(theta, prop_sd, aux, adapt_rate, n_per_adap):\n",
    "    accept_prob = np.zeros((n_per_adap, n_param))\n",
    "    for i in range(n_per_adap):\n",
    "        theta, accept_prob[i,:], aux \\\n",
    "            = metropolis_gibbs_step(theta, prop_sd, aux)\n",
    "    accept_rate = np.mean(accept_prob, 0)\n",
    "    prop_sd *= np.exp(adapt_rate * (accept_rate - .441))\n",
    "    return theta, prop_sd, accept_rate, aux\n",
    "\n",
    "def metropolis_gibbs_step(theta, prop_sd, aux):\n",
    "    accept_prob = np.zeros(n_param)\n",
    "    for index in range(n_param):\n",
    "        theta, accept_prob[index], aux = \\\n",
    "            cond_metropolis_update(theta, index, prop_sd, aux)\n",
    "    return theta, accept_prob, aux\n",
    "\n",
    "def cond_metropolis_update(theta, index, prop_sd, aux):\n",
    "    # Sample from the conditional distribution imitating the optimal\n",
    "    # Metropolis proposal standard deviation.\n",
    "    dtheta = prop_sd[index] * np.random.randn()\n",
    "    logp_diff, aux_new = f_update(theta, dtheta, index, aux)\n",
    "    accept_prob = min(1, math.exp(logp_diff))\n",
    "    if accept_prob > np.random.uniform():\n",
    "        theta[index] += dtheta\n",
    "        aux = aux_new\n",
    "    return theta, accept_prob, aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First adapt univariate proposal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adap = 10 ** 3\n",
    "prop_sd = 2.40 * np.ones(n_param)\n",
    "\n",
    "np.random.seed(seed)\n",
    "_, _, aux = f(theta)\n",
    "theta, accept_rate, prop_sd, aux \\\n",
    "    = adap_metropolis_gibbs(theta, prop_sd, aux, n_adap, n_per_adap=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the sampler with fixed proposal variances (for roughly the same amount of time as DHMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_sample = 25 * 10 ** 3\n",
    "\n",
    "samples = np.zeros((n_sample, n_param))\n",
    "accept_prob = np.zeros((n_sample, n_param))\n",
    "\n",
    "samples[0, :] = theta\n",
    "for i in range(1, n_sample):\n",
    "    samples[i, :], accept_prob[i, :], aux \\\n",
    "        = metropolis_gibbs_step(samples[i - 1, :], prop_sd, aux)\n",
    "        \n",
    "gibbs_samples = samples\n",
    "accept_rate = np.mean(accept_prob[1:,:], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ESS's and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_gibbs = batch_ess(gibbs_samples, normed=False)\n",
    "index_sort = np.argsort(ess_gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ess_gibbs[index_sort[:100]])\n",
    "plt.ylabel('ESS')\n",
    "plt.xlabel('Param index (sorted)')\n",
    "plt.title('ESS of 100 worst mixing params')\n",
    "plt.ylim(0, 1.05 * np.max(ess_gibbs[index_sort[:100]]))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "thin = 25\n",
    "plt.plot(gibbs_samples[::thin, index_sort[:3]])\n",
    "plt.ylabel('Param values')\n",
    "plt.xlabel('MCMC iteration (thin = {:d})'.format(thin))\n",
    "plt.title('Traceplot of 3 worst mixing params')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
