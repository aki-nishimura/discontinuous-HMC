{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "home_dir = '/Users/an88/'\n",
    "sys.path.insert(0, home_dir + 'Dropbox/Documents (Academic)/GitHub/NUTS (Python)/')\n",
    "sys.path.insert(0, home_dir + 'Dropbox/Documents (Academic)/GitHub/Discontinuous HMC/')\n",
    "sys.path.insert(0, home_dir + 'Dropbox/Documents (Academic)/GitHub/Discontinuous HMC/dhmc/')\n",
    "from importlib import reload\n",
    "from dhmc_sampler import DHMCSampler\n",
    "import NUTS\n",
    "from mcmc_diagnostic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "import pdb\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Jolly-Seber model with black-kneed capsid data from Seber (1982)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jolly_seber_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi0 = .8 * np.ones(len(index_phi))\n",
    "p0 = .15 * np.ones(len(index_p))\n",
    "U_init0 = 500\n",
    "B0 = 200 * np.ones(len(index_U) - 1)\n",
    "theta0 = pack_param(p0, phi0, U_init0, B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the coordinate wise update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.ones(n_param)\n",
    "dhmc = DHMCSampler(f, f_update, n_disc, n_param, scale)\n",
    "dhmc.test_cont_grad(theta0, sd=1, n_test=10);\n",
    "_, theta, logp_fdiff, logp_diff = \\\n",
    "    dhmc.test_update(theta0, sd=10, n_test=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an integrator for discontinuous HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(samples, logp_samples, accept_prob, nfevals_per_itr):\n",
    "    \n",
    "    filename = 'jolly_seber_test_output.py'\n",
    "    with open(filename, 'rb') as file:\n",
    "        samples0, logp_samples0, accept_prob0, nfevals_per_itr0 \\\n",
    "            = pkl.load(file)\n",
    "            \n",
    "    test_pass = np.allclose(samples, samples0) \\\n",
    "        and np.allclose(logp_samples, logp_samples0) \\\n",
    "        and np.allclose(accept_prob, accept_prob0) \\\n",
    "        and np.allclose(nfevals_per_itr, nfevals_per_itr0)\n",
    "        \n",
    "    if test_pass:\n",
    "        print('Test passed! The current output matches the former one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_burnin = 10 ** 1\n",
    "n_sample = 1 * 10 ** 1\n",
    "n_update = 1\n",
    "dt = .025 * np.array([.8, 1])\n",
    "nstep = [70, 85]\n",
    "\n",
    "samples, logp_samples, accept_prob, nfevals_per_itr, time_elapsed = \\\n",
    "    dhmc.run_sampler(theta0, dt, nstep, n_burnin, n_sample, \n",
    "                     seed=seed, n_update=n_update)\n",
    "samples = samples[n_burnin:, :]\n",
    "check_output(samples, logp_samples, accept_prob, nfevals_per_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_test_output = False\n",
    "if update_test_output:\n",
    "    filename = 'jolly_seber_test_output.py'\n",
    "    with open(filename, 'wb') as file:\n",
    "        to_save = (samples, logp_samples, accept_prob, nfevals_per_itr)\n",
    "        pkl.dump(to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output = {\n",
    "    'samples': samples,\n",
    "    'logp': logp_samples,\n",
    "    'accept_prob': accept_prob,\n",
    "    'nfevals_per_itr': nfevals_per_itr,\n",
    "    'n_burnin': n_burnin,\n",
    "    'seed': seed,\n",
    "    'theta0': theta0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_dhmc_output.npy'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(mcmc_output, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS-Gibbs sampler for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_U(theta):\n",
    "    \n",
    "    # Extract each parameter.\n",
    "    tilphi = theta[index_phi]\n",
    "    tilp = theta[index_p]\n",
    "    phi = 1 / (1 + np.exp(-tilphi))\n",
    "    p = 1 / (1 + np.exp(-tilp))\n",
    "    log_U = theta[index_U]\n",
    "    U = np.floor(np.exp(log_U)).astype('int64') \n",
    "    \n",
    "    for index in range(len(U)):\n",
    "        U[index] = cond_update_U(phi, p, U, index)\n",
    "    \n",
    "    theta[index_U] = np.log(U)\n",
    "    return theta\n",
    "\n",
    "def cond_update_U(phi, p, U, index):\n",
    "    \n",
    "    # The possible values of U[index].\n",
    "    U_i = np.arange(u[index], n_max)\n",
    "    \n",
    "    # Log-likelihood of the 1st capture.\n",
    "    logp = \\\n",
    "        log_factorial[U_i] - log_factorial[U_i - u[index]] + U_i * np.log(1 - p[index]) \n",
    "    \n",
    "    # Contributions from prior. Take advantage of Markovian structure.    \n",
    "    if index == 0:\n",
    "        logp += - np.log(U_i)\n",
    "    else:\n",
    "        U_prev_var = sigma_B ** 2 + \\\n",
    "            phi[index - 1] * (1 - phi[index - 1]) * (U - u)[index - 1]\n",
    "        logp += - np.log(U_prev_var) / 2 + \\\n",
    "            - (U_i - phi[index - 1] * (U[index - 1] - u[index - 1])) ** 2 / U_prev_var / 2\n",
    "    \n",
    "    if index < len(U) - 1:\n",
    "        U_i_var = sigma_B ** 2 + \\\n",
    "            phi[index] * (1 - phi[index]) * (U_i - u[index])\n",
    "        logp += - np.log(U_i_var) / 2 + \\\n",
    "            - (U[index + 1] - phi[index] * (U_i - u[index])) ** 2 / U_i_var / 2\n",
    "    \n",
    "    # Multi-nomial sampling\n",
    "    prob = np.exp(logp - np.max(logp))\n",
    "    prob = prob / np.sum(prob)\n",
    "    \n",
    "    return np.random.choice(U_i, size=1, p=prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The integrator for NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(f, epsilon, theta0, p0, grad0):\n",
    "\n",
    "    p = p0.copy()\n",
    "    theta = theta0.copy()\n",
    "    grad = grad0.copy()\n",
    "\n",
    "    p = p + 0.5 * epsilon * grad\n",
    "    theta = theta + epsilon * p\n",
    "    logp, grad = f(theta)\n",
    "    p = p + 0.5 * epsilon * grad\n",
    "    nfevals = 1\n",
    "\n",
    "    return theta, p, grad, logp, nfevals\n",
    "\n",
    "NUTS.integrator = leapfrog\n",
    "NUTS.compute_hamiltonian = lambda logp, p: logp - 0.5 * np.dot(p, p)\n",
    "NUTS.random_momentum = lambda d: np.random.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "n_burnin = 10 ** 3\n",
    "n_mcmc = 1 * 10 ** 3\n",
    "n_update = 10\n",
    "seed = 1\n",
    "\n",
    "# Run Gibbs with NUTS update for continuous variables\n",
    "theta = theta0.copy()\n",
    "n_per_update = math.ceil(n_mcmc / n_update)\n",
    "nfevals_total = 0\n",
    "samples = np.zeros((n_mcmc + n_burnin, len(theta)))\n",
    "logp_samples = np.zeros(n_mcmc + n_burnin)\n",
    "accept_prob = np.zeros(n_mcmc + n_burnin)\n",
    "\n",
    "np.random.seed(seed)\n",
    "def f_cond(theta_cont):\n",
    "    logp, grad, _ = f(np.concatenate((theta_cont, theta[n_cont:])))\n",
    "    if not np.any(np.isnan(grad)):\n",
    "        grad = grad[:n_cont]\n",
    "    return logp, grad\n",
    "logp, grad = f_cond(theta)\n",
    "for i in range(n_mcmc + n_burnin):\n",
    "    def f_cond(theta_cont):\n",
    "        logp, grad, _ = f(np.concatenate((theta_cont, theta[n_cont:])))\n",
    "        if not np.any(np.isnan(grad)):\n",
    "            grad = grad[:n_cont]\n",
    "        return logp, grad\n",
    "#     theta_cont, logp, grad, accept_prob[i], nfevals = \\\n",
    "#         NUTS.HMC(f_cond, np.random.uniform(2 * 10 ** -2, 2.5 * 10 ** -2), np.random.randint(50, 60), theta[:n_cont], logp, grad)\n",
    "    theta_cont, logp, grad, accept_prob[i], nfevals = \\\n",
    "        NUTS.NUTS(f_cond, np.random.uniform(2 * 10 ** -2, 2.5 * 10 ** -2), theta[:n_cont], logp, grad, max_depth=8)\n",
    "    theta[:n_cont] = theta_cont\n",
    "    theta = update_U(theta)\n",
    "    nfevals_total += nfevals + 1\n",
    "    samples[i, :] = theta\n",
    "    logp_samples[i] = logp\n",
    "    if (i + 1) % n_per_update == 0:\n",
    "        print('{:d} iterations have been completed.'.format(i+1))\n",
    "        \n",
    "nfevals_per_itr = nfevals_total / (n_mcmc + n_burnin)\n",
    "print('Each iteration required {:.2f} likelihood evaluations on average.'.format(nfevals_per_itr))\n",
    "samples = samples[n_burnin:, :]\n",
    "gibbs_samples = samples.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = None\n",
    "mcmc_output = {\n",
    "    'samples': samples,\n",
    "    'logp': logp_samples,\n",
    "    'accept_prob': accept_prob,\n",
    "    'nfevals_per_itr': nfevals_per_itr,\n",
    "    'n_burnin': n_burnin,\n",
    "    'seed': seed,\n",
    "    'theta0': theta0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_gibbs_output.npy'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(mcmc_output, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try M-H sampler with an optimal proposal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RWMH_step(f, theta0, logp0, stepsize, Sigma=None):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    f : function\n",
    "        Computes the log density of the target density\n",
    "    stepsize : scalar or vector\n",
    "        Proposal std is scaled by 'diag(stepsize).'\n",
    "    Sigma : matrix\n",
    "        Covariance matrix. If given, the proposal variance\n",
    "        will be 'stepsize ** 2 * Sigma.'\n",
    "    \"\"\"\n",
    "    theta = theta0.copy()\n",
    "    if Sigma is None:\n",
    "        theta += stepsize * np.random.randn(len(theta0))\n",
    "    else:\n",
    "        theta += stepsize * np.random.multivariate_normal(np.zeros(len(theta)), Sigma)\n",
    "    logp = f(theta)\n",
    "    accept_prob = min(1, math.exp(logp - logp0))\n",
    "    accepted = accept_prob > np.random.uniform()\n",
    "    if not accepted:\n",
    "        theta = theta0\n",
    "        logp = logp0\n",
    "        \n",
    "    return theta, logp, accepted, accept_prob\n",
    "\n",
    "def dual_average(mcmc_step, chain_state, stepsize, accept_target, n_iter):\n",
    "    \"\"\"\n",
    "    Adjusts the stepsize of the proposal kernel to achieve the target acceptance rate.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    mcmc_step : function(chain_state, stepsize)\n",
    "        Takes a dict of the current state and output a dict of the next\n",
    "        state. The dict must have a key 'accept_prob'.\n",
    "    stepsize : scalar\n",
    "        Initial stepsize to try.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    stepsize_bar : scalar\n",
    "        Adjusted stepsize \n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters to the dual averaging algorithm.\n",
    "    gamma = 0.1\n",
    "    t0 = 10\n",
    "    kappa = 0.75\n",
    "    mu = math.log(1. * stepsize)\n",
    "\n",
    "    # Initialize dual averaging algorithm.\n",
    "    stepsizebar = stepsize\n",
    "    \n",
    "    # Save the history of attempted and averaged stepsizes, which\n",
    "    # is useful mainly for checking the behavior of dual-averating.\n",
    "    stepsize_seq = np.zeros(n_iter + 1)\n",
    "    stepsizebar_seq = np.zeros(n_iter + 1)\n",
    "    stepsize_seq[0] = stepsize\n",
    "    stepsizebar_seq[0] = stepsizebar\n",
    "\n",
    "    Hbar = 0\n",
    "    for i in range(1, n_iter + 1):\n",
    "        chain_state = mcmc_step(chain_state, stepsize)\n",
    "        accept_prob = chain_state['accept_prob']\n",
    "        w = 1 / (i + t0)\n",
    "        Hbar = (1 - w) * Hbar + w * (accept_target - accept_prob)\n",
    "        log_stepsize = mu - math.sqrt(i) / gamma * Hbar\n",
    "        if log_stepsize < -100:\n",
    "            stepsize = 0\n",
    "        elif log_stepsize > 100:\n",
    "            stepsize = float('inf')\n",
    "        else:\n",
    "            stepsize = math.exp(log_stepsize)\n",
    "        eta = i ** -kappa\n",
    "        stepsizebar = math.exp((1 - eta) * math.log(stepsizebar) + eta * math.log(stepsize))\n",
    "        stepsize_seq[i] = stepsize\n",
    "        stepsizebar_seq[i] = stepsizebar\n",
    "\n",
    "    return chain_state, stepsizebar, stepsize_seq, stepsizebar_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RWMH(f, theta0, stepsize, n_warmup, n_samples, Sigma=None):\n",
    "    \"\"\" \n",
    "    Wrapper function for tuning the proposal variance and sampling\n",
    "    from the target using RWMH. \n",
    "    \"\"\"\n",
    "    \n",
    "    n_param = len(np.atleast_1d(theta0))\n",
    "    \n",
    "    # Tune the proposal variance using dual-averaging.\n",
    "    if n_warmup > 0:\n",
    "        def mcmc_step(chain_state, stepsize):\n",
    "            theta, logp, accepted, accept_prob = \\\n",
    "                RWMH_step(f, chain_state['theta'], chain_state['logp'], stepsize, Sigma)\n",
    "            return {'theta': theta, 'logp': logp, 'accept_prob': accept_prob}\n",
    "        init_state = {'theta': theta0, 'logp': f(theta0), 'accept_prob': 0}\n",
    "        chain_state, stepsize, stepsize_seq, ave_stepsize_seq = \\\n",
    "            dual_average(mcmc_step, init_state, stepsize, accept_target=.234, n_iter=n_warmup)\n",
    "        theta0 = chain_state['theta']\n",
    "    else:\n",
    "        stepsize_seq = None\n",
    "        ave_stepsize_seq = None\n",
    "        \n",
    "    mcmc_samples = np.zeros((n_samples, n_param))\n",
    "    accepted = np.zeros(n_samples)\n",
    "    mcmc_samples[0,:] = theta0\n",
    "    logp = f(theta0)\n",
    "    for i in range(1, n_samples):\n",
    "        mcmc_samples[i, :], logp, accepted[i], _ = \\\n",
    "            RWMH_step(f, mcmc_samples[i - 1, :], logp, stepsize, Sigma)\n",
    "    accept_rate = np.mean(accepted[1:])\n",
    "    \n",
    "    return mcmc_samples, accept_rate, stepsize_seq, ave_stepsize_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adap_RWMH(f, theta0, stepsize, n_warmup, n_cov_adap, n_samples):\n",
    "    \"\"\" \n",
    "    Params\n",
    "    ------\n",
    "    stepsize: float\n",
    "        Initial stepsize (proposal std)\n",
    "    n_warmup: int\n",
    "        Number of iterations for tuning the (isotropic) proposal variance\n",
    "    n_cov_adap: int\n",
    "        Number of iterations for estimating the covariance by running RWMH with \n",
    "        the tuned isotropic proposal variance.\n",
    "    n_samples: int\n",
    "        Number of sampling iteration during which the covariance will continue\n",
    "        to be \"diminishigly\" adapted.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_param = len(np.atleast_1d(theta0))\n",
    "    \n",
    "    # Tune the proposal variance using dual-averaging.\n",
    "    if n_warmup > 0:\n",
    "        def mcmc_step(chain_state, stepsize):\n",
    "            theta, logp, accepted, accept_prob = \\\n",
    "                RWMH_step(f, chain_state['theta'], chain_state['logp'], stepsize)\n",
    "            return {'theta': theta, 'logp': logp, 'accept_prob': accept_prob}\n",
    "        init_state = {'theta': theta0, 'logp': f(theta0), 'accept_prob': 0}\n",
    "        chain_state, stepsize, stepsize_seq, ave_stepsize_seq = \\\n",
    "            dual_average(mcmc_step, init_state, stepsize, accept_target=.234, n_iter=n_warmup)\n",
    "        theta0 = chain_state['theta']\n",
    "    \n",
    "    if n_cov_adap > 0:\n",
    "        cov_est_samples = np.zeros((n_samples, n_param))\n",
    "        cov_est_samples[0,:] = theta0\n",
    "        logp = f(theta0)\n",
    "        for i in range(1, n_samples):\n",
    "            cov_est_samples[i, :], logp, _, _ = \\\n",
    "                RWMH_step(f, cov_est_samples[i - 1, :], logp, stepsize)\n",
    "        mu_hat = np.mean(cov_est_samples, 0)\n",
    "        Sigma_hat = np.cov(cov_est_samples.T)\n",
    "        stepsize = 2.38 / np.sqrt(n_param)\n",
    "        \n",
    "    epsilon = 10 ** -3 \n",
    "    mcmc_samples = np.zeros((n_samples, n_param))\n",
    "    accepted = np.zeros(n_samples)\n",
    "    theta = theta0\n",
    "    mcmc_samples[0,:] = theta\n",
    "    logp = f(theta0)\n",
    "    for i in range(1, n_samples):\n",
    "        Sigma = (1 - epsilon) * stepsize * Sigma_hat + epsilon * stepsize * np.eye(n_param)\n",
    "        theta, logp, accepted[i], _ = \\\n",
    "            RWMH_step(f, theta, logp, stepsize, Sigma)\n",
    "        mu_hat, Sigma_hat = update_emp_cov(theta, mu_hat, Sigma_hat, i + n_cov_adap)\n",
    "        mcmc_samples[i, :] = theta\n",
    "    accept_rate = np.mean(accepted[1:])\n",
    "    \n",
    "    return mcmc_samples, accept_rate\n",
    "                          \n",
    "def update_emp_cov(theta, mu_hat, Sigma_hat, n):\n",
    "    Sigma_hat = (n - 1) / n * Sigma_hat + \\\n",
    "        (n - 1) / n ** 2 * np.outer(theta - mu_hat, theta - mu_hat)\n",
    "    mu_hat = (n - 1) / n * mu_hat + 1 / n * theta\n",
    "    return mu_hat, Sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "def f_logp(theta):\n",
    "    logp, _, _ = f(theta, req_grad=False)\n",
    "    return logp\n",
    "\n",
    "Sigma = 2.38 / np.sqrt(n_param) * np.cov(samples.T)\n",
    "stepsize = 1\n",
    "\n",
    "# Run MH with a fixed covariance.\n",
    "n_samples = 2 * 10 ** 3\n",
    "n_warmup = 0\n",
    "tic = time.process_time() # Start clock\n",
    "samples, accept_rate, stepsize_seq, ave_stepsize_seq = \\\n",
    "    RWMH(f_logp, theta0, stepsize, n_warmup, n_samples, Sigma)\n",
    "toc = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "def f_logp(theta):\n",
    "    logp, _, _ = f(theta, req_grad=False)\n",
    "    return logp\n",
    "\n",
    "# Run adaptive MH to estimate the covariance.\n",
    "n_warmup = 10 ** 4\n",
    "n_cov_adap = 10 ** 4\n",
    "n_adap_mcmc = 10 ** 3\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "stepsize = 1 / math.sqrt(n_param)\n",
    "samples, accept_rate = \\\n",
    "    Adap_RWMH(f_logp, theta0, stepsize, n_warmup, n_cov_adap, n_adap_mcmc)\n",
    "Sigma = 2.38 / np.sqrt(n_param) * np.cov(samples.T)\n",
    "\n",
    "# Run MH with a fixed covariance.\n",
    "n_samples = 5 * 10 ** 3\n",
    "tic = time.process_time() # Start clock\n",
    "samples, accept_rate, stepsize_seq, ave_stepsize_seq = \\\n",
    "    RWMH(f_logp, theta0, stepsize, n_warmup, n_samples, Sigma)\n",
    "toc = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output = {\n",
    "    'samples': samples,\n",
    "    'accept_rate': accept_rate,\n",
    "    'n_warmup': n_warmup,\n",
    "    'n_cov_adap': n_cov_adap,\n",
    "    'n_adap_mcmc': n_adap_mcmc,\n",
    "    'seed': seed,\n",
    "    'theta0': theta0,\n",
    "}\n",
    "filename = 'jolly_seber_mh_output.npy'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(mcmc_output, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the MCMC output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def coda_ess(samples, normed=True):\n",
    "    filenum = np.random.randint(2 ** 31) \n",
    "    # Append a random number to a file name to avoid conflicts.\n",
    "    saveto = 'mchain{:d}.csv'.format(filenum)\n",
    "    loadfrom = 'ess{:d}.csv'.format(filenum)\n",
    "    np.savetxt(saveto, samples, delimiter=',')\n",
    "    os.system(\" \".join([\"Rscript compute_ess.R\", saveto, loadfrom]))\n",
    "    ess = np.loadtxt(loadfrom, delimiter=',').copy()\n",
    "    if normed:\n",
    "        ess = ess / samples.shape[0]\n",
    "    os.system(\" \".join([\"rm\", saveto, loadfrom]))\n",
    "    return ess\n",
    "\n",
    "def mon_seq_ess(samples, normed=True):\n",
    "    ess = [compute_ess(column, normed=True)[0] for column in samples.T]\n",
    "    return np.squeeze(np.array(ess))\n",
    "\n",
    "def batch_ess(x, n_batch=50, normed=True):\n",
    "    batch_index = np.linspace(0, len(x), n_batch + 1).astype('int')\n",
    "    batch_list = [x[batch_index[i]:batch_index[i+1]] for i in range(n_batch)]\n",
    "    batch_mean = [np.mean(batch) for batch in batch_list]\n",
    "    mcmc_var = len(x) / n_batch * np.var(batch_mean)\n",
    "    ess = np.var(x) / mcmc_var\n",
    "    if not normed: ess *= len(x)\n",
    "    return ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_dhmc_output.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    mcmc_output = pkl.load(file)\n",
    "samples = mcmc_output['samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentist estimates.\n",
    "phi_hat = np.array([.649, 1.015, .867, .564, .836, .790, .651, .985, .686, .884, .771, float('nan')])\n",
    "phi_hat_sd = np.array([.114, .110, .107, .064, .075, .070, .056, .093, .080, .120, .128, float('nan')])\n",
    "N_hat = np.array([float('nan'), 511.2, 779.1, 963.0, 945.3, 882.2, 802.5, 653.6, 628.8, 478.5, 506.4, 462.8, float('nan')])\n",
    "N_hat_sd = np.array([float('nan'), 151.2, 129.3, 140.9, 125.5, 96.1, 74.8, 61.7, 61.9, 51.8, 65.8, 70.2, float('nan')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_samples, phi_samples, U_samples, N_samples = \\\n",
    "    unpack_param(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((np.mean(phi_samples, axis=0), phi_hat)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((np.std(phi_samples, axis=0), phi_hat_sd)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((np.mean(N_samples, 0), N_hat)).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((np.std(N_samples, 0), N_hat_sd)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(p_samples[:1000,:])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(p_samples[:, 0], bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(U_samples[:1000,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the correlation structure of the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_index = np.hstack(tuple([index_U[i], index_p[i]] for i in range(T - 1)))\n",
    "plt.imshow(np.corrcoef(samples[:, param_index].T), cmap='coolwarm')\n",
    "plt.clim(-1, 1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_index = np.hstack(tuple([index_U[i], index_phi[i]] for i in range(T - 1)))\n",
    "plt.imshow(np.corrcoef(samples[:, param_index].T), cmap='coolwarm')\n",
    "plt.clim(-1, 1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.corrcoef(U_samples.T), cmap='coolwarm')\n",
    "plt.xlabel('')\n",
    "plt.clim(-1, 1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(p_samples[:,0], np.log(U_samples[:,0]), bins=20, cmap='inferno')\n",
    "plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.hist2d(logodd(p_samples[:,0]), np.log10(U_samples[:,0]), bins=20, normed=True, cmap='inferno')\n",
    "plt.xlabel(r'$\\log(q_1 / (1 - q_1))$')\n",
    "plt.ylabel(r'$\\log_{10}(N_1)$')\n",
    "plt.colorbar(ticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('jolly_seber_posterior_2dhist.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(np.log10(U_samples[:,2]), logit(phi_samples[:,0]), bins=20, cmap='inferno')\n",
    "# plt.hist2d(logit(p_samples[:,0]), np.log(N_samples[:,0]), bins=20, cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_index = np.hstack(tuple([index_phi[i]] for i in range(T - 1)))\n",
    "plt.imshow(np.corrcoef(samples[:, param_index].T), cmap='coolwarm')\n",
    "plt.clim(-1, 1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
