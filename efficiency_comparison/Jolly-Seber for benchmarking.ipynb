{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from dhmc.dhmc_sampler import DHMCSampler\n",
    "from benchmarking_util import summarize_sim_results\n",
    "    # Utility functions to summarize the simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions to compute the posterior of the Jolly-Seber model based on the black-kneed capsid data from Seber (1982)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_and_posterior.jolly_seber_model \\\n",
    "    import f, f_update\n",
    "from data_and_posterior.jolly_seber_model \\\n",
    "    import pack_param, unpack_param, index, n_param, n_disc, n_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the gradient and coordinatewise update function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from stationary point.\n",
    "U0 = np.array([299, 371, 375, 436, 690, 480, 404, 619, 187, 163, 196, 261, 464])\n",
    "phi0 = np.array([.67, .87, .92, .54, .76, .90, .63, .95, .88, .92, .96, .95])\n",
    "p0 = np.array([.28, .44, .35, .33, .21, .29, .36, .24, .35, .24, .20, .20, .13])\n",
    "theta0 = pack_param(p0, phi0, U0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.ones(n_param)\n",
    "dhmc = DHMCSampler(f, f_update, n_disc, n_param, scale)\n",
    "dhmc.test_cont_grad(theta0, sd=1, n_test=10);\n",
    "_, theta, logp_diff, logp_diff_update = \\\n",
    "    dhmc.test_update(theta0, sd=.1, n_test=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DHMC with an identity mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 10 ** 3\n",
    "n_sample = 1 * 10 ** 4\n",
    "n_update = 10\n",
    "dt = .025 * np.array([.8, 1])\n",
    "nstep = [70, 85]\n",
    "\n",
    "def dhmc_simulation(seed):\n",
    "    samples, logp_samples, accept_prob, nfevals_per_itr, time_elapsed = \\\n",
    "        dhmc.run_sampler(theta0, dt, nstep, n_burnin, n_sample, seed=seed)\n",
    "    samples = samples[n_burnin:, :]\n",
    "    logp_samples = logp_samples[n_burnin:]\n",
    "    time_elapsed *= n_sample / (n_sample + n_burnin) # Adjust for the burn-in time.\n",
    "    summary = summarize_sim_results(\n",
    "        samples, time_elapsed, nfevals_per_itr, n_sample, n_burnin, theta0, seed\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = Parallel(n_jobs=4)(delayed(dhmc_simulation)(i) for i in range(n_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_dhmc_simulation.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(sim_result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DHMC with a diaognal mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_dhmc_output.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    mcmc_output = pkl.load(file)\n",
    "scale = np.std(mcmc_output['samples'], 0)\n",
    "scale /= np.max(scale)\n",
    "dhmc = DHMCSampler(f, f_update, n_disc, n_param, scale)\n",
    "\n",
    "n_burnin = 10 ** 3\n",
    "n_sample = 1 * 10 ** 4\n",
    "n_update = 10\n",
    "dt = .175 * np.array([.8, 1])\n",
    "nstep = [40, 50]\n",
    "\n",
    "def dhmc_simulation(seed):\n",
    "    samples, logp_samples, accept_prob, nfevals_per_itr, time_elapsed = \\\n",
    "        dhmc.run_sampler(theta0, dt, nstep, n_burnin, n_sample, seed=seed)\n",
    "    samples = samples[n_burnin:, :]\n",
    "    logp_samples = logp_samples[n_burnin:]\n",
    "    time_elapsed *= n_sample / (n_sample + n_burnin) # Adjust for the burn-in time.\n",
    "    return summarize_sim_results(samples, time_elapsed, nfevals_per_itr, \n",
    "                                 n_sample, n_burnin, theta0, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = Parallel(n_jobs=4)(delayed(dhmc_simulation)(i) for i in range(n_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_adap_dhmc_simulation.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(sim_result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs + NUTS sampler for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_and_posterior.jolly_seber_model import update_disc\n",
    "from other_samplers.nuts_sampler import nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi0 = .8 * np.ones(len(index[\"phi\"]))\n",
    "p0 = .15 * np.ones(len(index[\"p\"]))\n",
    "U0 = 500 * np.ones(len(index[\"U\"]))\n",
    "theta0 = pack_param(p0, phi0, U0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_gibbs(f, theta, dt, logp, grad, max_depth):\n",
    "    def f_cond(theta_cont):\n",
    "        logp, grad, _ = f(np.concatenate((theta_cont, theta[n_cont:])))\n",
    "        if not np.any(np.isnan(grad)):\n",
    "            grad = grad[:n_cont]\n",
    "        return logp, grad\n",
    "    theta_cont, logp, grad, nuts_accept_prob, nfevals = \\\n",
    "        nuts(f_cond, np.random.uniform(dt[0], dt[1]), theta[:n_cont], logp, grad, max_depth, warnings=False)\n",
    "    theta[:n_cont] = theta_cont\n",
    "    theta = update_disc(theta)\n",
    "    logp, grad, _ = f(theta)\n",
    "    grad = grad[:n_cont]    \n",
    "    nfevals += 1\n",
    "    return theta, logp, grad, nuts_accept_prob, nfevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 10 ** 3\n",
    "n_sample = 1 * 10 ** 4\n",
    "n_update = 1\n",
    "dt = .025 * np.array([.8, 1]) # Same as DHMC.\n",
    "\n",
    "# Run Gibbs with NUTS update for continuous variable.\n",
    "def nuts_gibbs_simulation(seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Pre-allocate\n",
    "    theta = theta0.copy()\n",
    "    n_per_update = math.ceil((n_sample + n_burnin) / n_update)\n",
    "    nfevals_total = 0\n",
    "    samples = np.zeros((n_sample + n_burnin, len(theta)))\n",
    "    logp_samples = np.zeros(n_sample + n_burnin)\n",
    "    accept_prob = np.zeros(n_sample + n_burnin)\n",
    "    \n",
    "    # Run NUTS-Gibbs\n",
    "    tic = time.process_time()\n",
    "    logp, grad, _ = f(theta)\n",
    "    grad = grad[:n_cont]\n",
    "    for i in range(n_sample + n_burnin):\n",
    "        theta, logp, grad, accept_prob[i], nfevals = \\\n",
    "            nuts_gibbs(f, theta, dt, logp, grad, max_depth=8)\n",
    "        nfevals_total += nfevals + 1\n",
    "        samples[i, :] = theta\n",
    "        logp_samples[i] = logp\n",
    "        if (i + 1) % n_per_update == 0:\n",
    "            print('{:d} iterations have been completed.'.format(i+1))\n",
    "\n",
    "    toc = time.process_time()\n",
    "    time_elapsed = toc - tic\n",
    "    time_elapsed *= n_sample / (n_sample + n_burnin) # Adjust for the burn-in time.  \n",
    "    nfevals_per_itr = nfevals_total / (n_sample + n_burnin)\n",
    "    print('Each iteration required {:.2f} likelihood evaluations on average.'.format(nfevals_per_itr))\n",
    "    \n",
    "    samples = samples[n_burnin:, :]\n",
    "    logp_samples = logp_samples[n_burnin:]\n",
    "    \n",
    "    return summarize_sim_results(samples, time_elapsed, nfevals_per_itr, \n",
    "                                 n_sample, n_burnin, theta0, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = Parallel(n_jobs=4)(delayed(nuts_gibbs_simulation)(i) for i in range(n_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_gibbs_simulation.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(sim_result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try M-H sampler with an optimal proposal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from other_samplers.adaptive_metropolis import adap_RWMH, RWMH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_logp(theta):\n",
    "    logp, _, _ = f(theta, req_grad=False)\n",
    "    return logp\n",
    "\n",
    "n_warmup = 10 ** 4\n",
    "n_cov_adap = 10 ** 4\n",
    "n_adap_mcmc = 5 * 10 ** 5\n",
    "n_sample = 5 * 10 ** 5\n",
    "\n",
    "def mh_simulation(seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Run adaptive MH to estimate the covariance.\n",
    "    stepsize = 2.38 / math.sqrt(n_param)\n",
    "    samples, accept_rate = \\\n",
    "        adap_RWMH(f_logp, theta0, stepsize, n_warmup, n_cov_adap, n_adap_mcmc)\n",
    "    Sigma = np.cov(samples.T)\n",
    "\n",
    "    # Run MH with a fixed covariance.\n",
    "    tic = time.process_time() # Start clock\n",
    "    samples, accept_rate, stepsize_seq, ave_stepsize_seq = \\\n",
    "        RWMH(f_logp, theta0, stepsize, 0, n_sample, Sigma)\n",
    "\n",
    "    toc = time.process_time()\n",
    "    time_elapsed = toc - tic\n",
    "    print('Sampling completed.')\n",
    "    \n",
    "    n_burnin = n_warmup + n_cov_adap + n_adap_mcmc\n",
    "    nfevals_per_itr = 1\n",
    "    return summarize_sim_results(samples, time_elapsed, nfevals_per_itr, \n",
    "                                 n_sample, n_burnin, theta0, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = Parallel(n_jobs=4)(delayed(mh_simulation)(i) for i in range(n_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'jolly_seber_mh_simulation.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pkl.dump(sim_result, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
